# ğŸ§  Chapter 16: Natural Language Processing with RNNs and Attention

_Buku: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow â€” AurÃ©lien GÃ©ron_

---

## ğŸ“Œ Apa Itu NLP?

Natural Language Processing (NLP) adalah cabang AI yang berfokus pada:
- ğŸ“„ Pemrosesan teks
- ğŸ™ï¸ Pemahaman ucapan
- ğŸ§  Interaksi bahasa alami antara manusia dan mesin

---

## ğŸ” Mengapa RNN Cocok untuk NLP?

- RNN dirancang untuk memproses **urutan data** (seperti kalimat).
- Dapat menyimpan **konteks temporal** dan ketergantungan antar kata.
- Cocok untuk:
  - Penerjemahan otomatis
  - Analisis sentimen
  - Generasi teks

---

## ğŸ§± Representasi Teks

| Teknik                | Penjelasan                                                                 |
|----------------------|----------------------------------------------------------------------------|
| ğŸ§© **Tokenisasi**    | Memecah teks menjadi kata/sub-kata/karakter.                               |
| ğŸ”¤ **Embedding**     | Mengubah token menjadi vektor numerik berdimensi tetap.                     |
| ğŸš« **OOV Handling**  | Sub-word tokenization mengurangi kasus _out-of-vocabulary_.                 |

---

## ğŸ”ğŸ§¾ Sequence-to-Sequence (Seq2Seq)

Model encoder-decoder berbasis RNN:
- **Encoder**: Membaca seluruh input sequence â†’ menghasilkan _context vector_.
- **Decoder**: Menghasilkan sequence output berdasarkan context tersebut.

âœ… Cocok untuk:
- Penerjemahan otomatis
- Chatbot
- Ringkasan teks

---

## ğŸ¯ Attention Mechanism

- **Masalah**: Context vector tunggal membatasi model pada sequence panjang.
- **Solusi**: Attention memberikan bobot pada tiap bagian input saat menghasilkan tiap token output.

### ğŸ“Œ Manfaat Attention:
- Fokus dinamis pada bagian penting input
- Meningkatkan kualitas output
- Mempercepat konvergensi pelatihan

---

## ğŸ§² Transformer dan Self-Attention

Transformer = model berbasis **attention penuh**, tanpa RNN.

### ğŸ” Self-Attention
- Setiap token memperhatikan **semua token lain**.
- Efektif untuk menangkap dependensi jangka panjang.

### ğŸš€ Dampak Transformer:
- Fondasi dari model seperti:
  - BERT
  - GPT
  - T5
- Mendominasi NLP modern

---

## ğŸ”„ Transfer Learning & Pretrained Models

- Model besar dilatih pada data masif (pretraining).
- Dapat di-*fine-tune* untuk tugas spesifik (transfer learning).
- Efektif bahkan dengan data spesifik yang terbatas.

---

## âš™ï¸ Teknik Pelatihan

| Teknik               | Penjelasan                                                                    |
|----------------------|--------------------------------------------------------------------------------|
| ğŸ” **BPTT**         | Backpropagation Through Time â€” digunakan untuk RNN.                            |
| ğŸ“ **Teacher Forcing** | Menggunakan target aktual sebagai input decoder saat pelatihan.               |

---

## ğŸ“š Aplikasi NLP

- ğŸŒ Penerjemahan Mesin (Machine Translation)
- ğŸ¤– Chatbots & Virtual Assistants
- ğŸ’¬ Analisis Sentimen
- ğŸ—£ï¸ Speech Recognition
- ğŸ“‘ Text Summarization

---

## âœ… Ringkasan

- RNN berguna untuk urutan, tapi **attention** melampaui batasannya.
- **Transformer** mempercepat dan menyederhanakan arsitektur NLP modern.
- Transfer learning dengan **pretrained language models** adalah kunci keberhasilan NLP saat ini.

---
